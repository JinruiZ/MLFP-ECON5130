\documentclass[10pt]{scrartcl}

    \input{overrides-pre}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \newcommand*{\mytitle}{Unit 11: Applications -- Econometrics}
    
    
    \input{overrides-post}
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,fontsize=\small,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    \tableofcontents

    

    
    \hypertarget{applications-econometrics}{%
\section{Applications: Econometrics}\label{applications-econometrics}}

In this unit, we study a few more advanced examples of how to use Python
to perform common econometric tasks. We will go beyond just calling
existing functions that someone implemented, but we instead will
implement these ourselves to understand the underlying concepts.

    \hypertarget{preliminaries-drawing-bivariate-samples}{%
\subsection{Preliminaries: Drawing bivariate
samples}\label{preliminaries-drawing-bivariate-samples}}

In most of the exercises below, we'll need to draw a random sample that
serves as an input. We therefore first define a routine which returns a
sample drawn from a bivariate normal distribution.

In line with what we learned in unit 10, we check arguments and raise an
exception if a an invalid value is encountered.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{random} \PY{k+kn}{import} \PY{n}{default\PYZus{}rng}

\PY{k}{def} \PY{n+nf}{draw\PYZus{}bivariate\PYZus{}sample}\PY{p}{(}\PY{n}{mean}\PY{p}{,} \PY{n}{std}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{n}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{123}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Draw a bivariate normal random sample.}

\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    mean : array\PYZus{}like}
\PY{l+s+sd}{        Length\PYZhy{}2 array of means}
\PY{l+s+sd}{    std : array\PYZus{}like}
\PY{l+s+sd}{        Length\PYZhy{}2 array of standard deviations}
\PY{l+s+sd}{    rho : float}
\PY{l+s+sd}{        Correlation parameter}
\PY{l+s+sd}{    n : int}
\PY{l+s+sd}{        Sample size}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{k}{if} \PY{o+ow}{not} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{rho} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{1}\PY{p}{:}
        \PY{k}{raise} \PY{n+ne}{ValueError}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Invalid correlation parameter: }\PY{l+s+si}{\PYZob{}}\PY{n}{rho}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{any}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{std}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
        \PY{k}{raise} \PY{n+ne}{ValueError}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Invalid standard deviation: }\PY{l+s+si}{\PYZob{}}\PY{n}{std}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{k}{if} \PY{n}{n} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{k}{raise} \PY{n+ne}{ValueError}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Invalid sample size: }\PY{l+s+si}{\PYZob{}}\PY{n}{n}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} initialize default RNG with given seed}
    \PY{n}{rng} \PY{o}{=} \PY{n}{default\PYZus{}rng}\PY{p}{(}\PY{n}{seed}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Unpack standard deviations for each dimension}
    \PY{n}{std1}\PY{p}{,} \PY{n}{std2} \PY{o}{=} \PY{n}{std}

    \PY{c+c1}{\PYZsh{} Compute covariance}
    \PY{n}{cov} \PY{o}{=} \PY{n}{rho} \PY{o}{*} \PY{n}{std1} \PY{o}{*} \PY{n}{std2}

    \PY{c+c1}{\PYZsh{} Create variance\PYZhy{}covariance matrix}
    \PY{n}{vcv} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{n}{std1}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{n}{cov}\PY{p}{]}\PY{p}{,}
                    \PY{p}{[}\PY{n}{cov}\PY{p}{,} \PY{n}{std2}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{2.0}\PY{p}{]}\PY{p}{]}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Draw MVN random numbers:}
    \PY{c+c1}{\PYZsh{} each row represents one sample draw.}
    \PY{n}{X} \PY{o}{=} \PY{n}{rng}\PY{o}{.}\PY{n}{multivariate\PYZus{}normal}\PY{p}{(}\PY{n}{mean}\PY{o}{=}\PY{n}{mean}\PY{p}{,} \PY{n}{cov}\PY{o}{=}\PY{n}{vcv}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{n}\PY{p}{)}

    \PY{k}{return} \PY{n}{X}
\end{Verbatim}
\end{tcolorbox}


    \hypertarget{singular-value-decomposition-svd-and-principal-components}{%
\subsection{Singular value decomposition (SVD) and principal
components}\label{singular-value-decomposition-svd-and-principal-components}}

Singular value decomposition is a matrix factorisation that is commonly
used in econometrics and statistics. For example, we can use it to
implement principal component analysis (PCA), principal component
regression, OLS or Ridge regression.

Let \(\mathbf{X} \in \mathbb{R}^{m\times n}\) be a matrix. For our
purposes, we will assume that \(m \geq n\) since \(\mathbf{X}\) will be
the matrix containing the data with observations in rows and variables
in column. The (compact) SVD of \(\mathbf{X}\) is given by
\[\mathbf{X} = \mathbf{U} \Sigma \mathbf{V}'\] where
\(\mathbf{U} \in \mathbb{R}^{m\times n}\) and
\(\mathbf{V} \in \mathbb{R}^{n\times n}\) are orthogonal matrices, and
\(\Sigma \in \mathbb{R}^{n \times n}\) is a diagonal matrix
\[\Sigma =  \begin{bmatrix} 
    \sigma_1 & & & & \\
     & \sigma_2 & & & \\
     & & \ddots & & \\
     & & & \sigma_n & 
\end{bmatrix}\] The elements \(\sigma_i\) are called singular values of
\(\mathbf{X}\), and \(\Sigma\) is arranged such that
\(\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_n\). Since
\(\mathbf{U}\) is not necessarily square, it's not truly orthogonal, but
its columns are still orthogonal to each other.

These matrices satisfy the following useful properties: \[
\def\bV{\mathbf{V}}
\def\bU{\mathbf{U}}
\def\bI{\mathbf{I}}
\begin{aligned}
    \bU' \bU &= \bI_n \\
    \bV' \bV &= \bV\bV' = \bI_n \\
    \bV' &= \bV^{-1}
\end{aligned}
\]

In Python, we compute the SVD using the
\href{https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html}{\texttt{svd()}}
function from \texttt{numpy.linalg}.

    \hypertarget{example-bivariate-normal}{%
\subsubsection{Example: Bivariate
normal}\label{example-bivariate-normal}}

Imagine we construct \(X\) as 200 random draws from a bivariate normal:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{random} \PY{k+kn}{import} \PY{n}{default\PYZus{}rng}

\PY{c+c1}{\PYZsh{} Draw a bivariate normal sample using the function we defined above}
\PY{n}{mu} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}         \PY{c+c1}{\PYZsh{} Vector of means}
\PY{n}{sigma} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}      \PY{c+c1}{\PYZsh{} Vector of standard deviations}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.75}              \PY{c+c1}{\PYZsh{} Correlation coefficient}
\PY{n}{Nobs} \PY{o}{=} \PY{l+m+mi}{200}              \PY{c+c1}{\PYZsh{} Sample size}
\PY{n}{X} \PY{o}{=} \PY{n}{draw\PYZus{}bivariate\PYZus{}sample}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{sigma}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{Nobs}\PY{p}{)}
\PY{n}{x1}\PY{p}{,} \PY{n}{x2} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{T}

\PY{c+c1}{\PYZsh{} Scatter plot of sample}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{x2}\PY{p}{,} \PY{n}{linewidths}\PY{o}{=}\PY{l+m+mf}{0.75}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{none}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{edgecolors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{steelblue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}x\PYZus{}1\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}x\PYZus{}2\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Draws from bivariate normal distribution}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, 'Draws from bivariate normal distribution')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{unit11_files/unit11_6_1.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can now perform the SVD as follows:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k+kn}{import} \PY{n}{svd}

\PY{c+c1}{\PYZsh{} svd() returns transposed V!}
\PY{c+c1}{\PYZsh{} We use full\PYZus{}matrices=False to get the compact factorisation, otherwise}
\PY{c+c1}{\PYZsh{} U is 200 x 200.}
\PY{n}{U}\PY{p}{,} \PY{n}{S}\PY{p}{,} \PY{n}{Vt} \PY{o}{=} \PY{n}{svd}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{full\PYZus{}matrices}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Check that U\PYZsq{}U is a 2x2 identity matrix}
\PY{n}{U}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{U}         \PY{c+c1}{\PYZsh{} or np.dot(U.T, U)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[ 1.00000000e+00, -6.12878704e-17],
       [-6.12878704e-17,  1.00000000e+00]])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Check that V\PYZsq{}V = VV\PYZsq{} is a 2x2 identity matrix}
\PY{n}{Vt}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{Vt}       \PY{c+c1}{\PYZsh{} or np.dot(Vt.T, Vt)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[ 1.00000000e+00, -2.26167254e-18],
       [-2.26167254e-18,  1.00000000e+00]])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} svd() does not return S as a matrix but only its diagonal!}
\PY{n}{S}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([19.73152572,  5.99498933])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} We can convert it to a diagonal matrix using np.diag()}
\PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{S}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[19.73152572,  0.        ],
       [ 0.        ,  5.99498933]])
\end{Verbatim}
\end{tcolorbox}
        
    Finally, we can multiply the output of \texttt{svd()} to verify that the
result is equal to \(\mathbf{X}\):

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}svd} \PY{o}{=} \PY{n}{U} \PY{o}{*} \PY{n}{S} \PY{o}{@} \PY{n}{Vt}

\PY{c+c1}{\PYZsh{} Compute the max. absolute difference}
\PY{n}{diff} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{amax}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{X} \PY{o}{\PYZhy{}} \PY{n}{X\PYZus{}svd}\PY{p}{)} \PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Max. absolute difference between X and USV}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{diff}\PY{l+s+si}{:}\PY{l+s+s2}{.2e}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Max. absolute difference between X and USV': 2.89e-15
    \end{Verbatim}

    \hypertarget{example-principal-components}{%
\subsubsection{Example: Principal
components}\label{example-principal-components}}

We use principal component analysis (PCA) as a dimension reduction
technique, which allows us to identify an alternate set of axes along
which the data in \(\mathbf{X}\) varies the most. In machine learning,
PCA is one of the most basic unsupervised learning techniques.

To perform the PCA, it is recommended to first demean the data:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X} \PY{o}{=} \PY{n}{draw\PYZus{}bivariate\PYZus{}sample}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{sigma}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{Nobs}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Demean variables in X}
\PY{n}{Xmean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Matrix Z stores the demeaned variables}
\PY{n}{Z} \PY{o}{=} \PY{n}{X} \PY{o}{\PYZhy{}} \PY{n}{Xmean}\PY{p}{[}\PY{k+kc}{None}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    We can now use the SVD factorisation to compute the principal
components. Once we have computed the matrices \(\mathbf{U}\),
\(\Sigma\) and \(\mathbf{V}\), the matrix of principal components (one
in each column) is given by \[
PC = \mathbf{U} \Sigma
\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Apply SVD to standardised values}
\PY{n}{U}\PY{p}{,} \PY{n}{S}\PY{p}{,} \PY{n}{Vt} \PY{o}{=} \PY{n}{svd}\PY{p}{(}\PY{n}{Z}\PY{p}{,} \PY{n}{full\PYZus{}matrices}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Compute principal components}
\PY{n}{PC} \PY{o}{=} \PY{n}{U} \PY{o}{*} \PY{n}{S}          \PY{c+c1}{\PYZsh{} same as U @ np.diag(S)}

\PY{c+c1}{\PYZsh{} Variance is highest for first component}
\PY{n}{var\PYZus{}PC} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{PC}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{ddof}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Principal component variances: }\PY{l+s+si}{\PYZob{}}\PY{n}{var\PYZus{}PC}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Principal component variances: [1.17607859 0.09444617]
    \end{Verbatim}

    We can plot the principal component axes in the original data space
(left columns). Moreover, the right column shows the data rotated and
rescaled so that each axes corresponds to a principal component. Most of
the variation clearly occurs along the first axis!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot principal components}

\PY{c+c1}{\PYZsh{} Scatter plot of sample}
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{linewidths}\PY{o}{=}\PY{l+m+mf}{0.75}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{none}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{edgecolors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{steelblue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{equal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}x\PYZus{}1\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}x\PYZus{}2\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{axline}\PY{p}{(}\PY{n}{Xmean}\PY{p}{,} \PY{n}{Xmean} \PY{o}{+} \PY{n}{Vt}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{zorder}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{axline}\PY{p}{(}\PY{n}{Xmean}\PY{p}{,} \PY{n}{Xmean} \PY{o}{+} \PY{n}{Vt}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{zorder}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{PC\PYZus{}arrows} \PY{o}{=} \PY{n}{Vt} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{var\PYZus{}PC}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}
\PY{k}{for} \PY{n}{v} \PY{o+ow}{in} \PY{n}{PC\PYZus{}arrows}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Scale up arrows by 3 so that they are visible!}
    \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{Xmean} \PY{o}{+} \PY{n}{v}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{Xmean}\PY{p}{,} \PY{n}{arrowprops}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{n}{arrowstyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZgt{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}

\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot in principal component coordinate system}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{PC}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{PC}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{linewidths}\PY{o}{=}\PY{l+m+mf}{0.75}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{none}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{edgecolors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{steelblue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{equal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.lines.Line2D at 0x7fc5a4254580>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{unit11_files/unit11_20_1.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    Of course, in real applications we don't need to manually compute the
principal components, but can use a library such as
\href{https://scikit-learn.org/stable/}{scikit-learn} to do it for us:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k+kn}{import} \PY{n}{PCA}

\PY{c+c1}{\PYZsh{} Draw the same sample as before}
\PY{n}{X} \PY{o}{=} \PY{n}{draw\PYZus{}bivariate\PYZus{}sample}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{sigma}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{Nobs}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Create PCA with 2 components (which is the max, since we have only two }
\PY{c+c1}{\PYZsh{} variables)}
\PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Perform PCA on input data}
\PY{n}{pca}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\PY{c+c1}{\PYZsh{} The attribute components\PYZus{} can be used to retrieve the V\PYZsq{} matrix}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Principal components (matrix V}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{):}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{components\PYZus{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} The attribute explained\PYZus{}variance\PYZus{} stores the variances of all PCs}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variance of each PC: }\PY{l+s+si}{\PYZob{}}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Fraction of variance explained by each component:}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fraction of variance of each PC: }\PY{l+s+si}{\PYZob{}}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Principal components (matrix V'):
[[ 0.38420018  0.92324981]
 [ 0.92324981 -0.38420018]]
Variance of each PC: [1.17607859 0.09444617]
Fraction of variance of each PC: [0.92566365 0.07433635]
    \end{Verbatim}


    \hypertarget{ordinary-least-squares-ols}{%
\subsection{Ordinary least squares
(OLS)}\label{ordinary-least-squares-ols}}

Consider the regression \[
y_i = \mathbf{x}_i' \beta + u_i
\] where \(\mathbf{x}_i\) is a vector of regressors (explanatory
variables) that is assumed to include a constant. Recall that the OLS
estimator \(\widehat{\beta}\) is given by \[
\def\bX{\mathbf{X}}
\def\bY{\mathbf{y}}
\widehat{\beta} = \left(\bX'\bX\right)^{-1}\bX'\bY
\] where \(\mathbf{X}\) is the regressor matrix that contains all
stacked \(\mathbf{x}_i'\), and \(\mathbf{y}\) contains all observations
of the dependent variable.

    \hypertarget{example-1-bivariate-data}{%
\subsubsection{Example 1: Bivariate
data}\label{example-1-bivariate-data}}

We first demonstrate how to run OLS using bivariate normal data. With
only one regressor, the regression simplifies to \[
y_i = \alpha + \beta x_i + u_i
\] where \(\alpha\) is the intercept and \(\beta\) is the slope
coefficient. In this special case, the population coefficient \(\beta\)
can be computed using the formula \[
\beta = \frac{E[(Y-\overline{Y})(X-\overline{X})]}{E[(X-\overline{X})]}
 = \frac{Cov(Y,X)}{Var(X)}
\] where the numerator contains the covariance of the random variables
\(Y\) and \(X\), and the denominator contains the variance of \(X\).
Given a sample of values, the estimator \(\widehat{\beta}\) is computed
using the corresponding sample moments: \[
\widehat{\beta} = \frac{\widehat{Cov}(y,x)}{\widehat{Var}(x)}
\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{n}{mu} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}                \PY{c+c1}{\PYZsh{} Mean of X and Y}
\PY{n}{std} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{1.5}\PY{p}{]}                \PY{c+c1}{\PYZsh{} Std. dev. of X and Y}
\PY{n}{rho} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}                      \PY{c+c1}{\PYZsh{} Correlation coefficient}
\PY{n}{Nobs} \PY{o}{=} \PY{l+m+mi}{100}                      \PY{c+c1}{\PYZsh{} Sample size}

\PY{c+c1}{\PYZsh{} We transpose the return value and unpack individual rows into X and Y}
\PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{draw\PYZus{}bivariate\PYZus{}sample}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{std}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{Nobs}\PY{p}{)}\PY{o}{.}\PY{n}{T}

\PY{c+c1}{\PYZsh{} Compute beta (slope coefficient) from distribution moments.}
\PY{c+c1}{\PYZsh{} This is the true underlying relationship given our data generating process.}
\PY{n}{cov} \PY{o}{=} \PY{n}{rho} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{prod}\PY{p}{(}\PY{n}{std}\PY{p}{)}
\PY{n}{beta} \PY{o}{=} \PY{n}{cov} \PY{o}{/} \PY{n}{std}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mf}{2.0}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Slope of population regression line: }\PY{l+s+si}{\PYZob{}}\PY{n}{beta}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Compute beta from sample moments}
\PY{c+c1}{\PYZsh{} Sample variance\PYZhy{}covariance matrix (ddof=1 returns the unbiased estimate)}
\PY{n}{cov\PYZus{}hat} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{cov}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{ddof}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{var\PYZus{}x\PYZus{}hat} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{ddof}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{beta\PYZus{}hat} \PY{o}{=} \PY{n}{cov\PYZus{}hat} \PY{o}{/} \PY{n}{var\PYZus{}x\PYZus{}hat}
\PY{c+c1}{\PYZsh{} Sample intercept}
\PY{n}{alpha\PYZus{}hat} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{beta\PYZus{}hat} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{x}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Slope of sample regression line: }\PY{l+s+si}{\PYZob{}}\PY{n}{beta\PYZus{}hat}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Scatter plot of sample}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{linewidths}\PY{o}{=}\PY{l+m+mf}{0.75}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{none}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{edgecolors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{steelblue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}x\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}y\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Draws from bivariate normal distribution}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{axline}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{slope}\PY{o}{=}\PY{n}{beta}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Regression line (population)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{axline}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{alpha\PYZus{}hat}\PY{p}{)}\PY{p}{,} \PY{n}{slope}\PY{o}{=}\PY{n}{beta\PYZus{}hat}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Regression line (sample)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Slope of population regression line: -1.5
Slope of sample regression line: -1.3889613032802288
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.legend.Legend at 0x7fc5713f1eb0>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{unit11_files/unit11_26_2.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{example-2-ols-using-matrix-algebra}{%
\subsubsection{Example 2: OLS using matrix
algebra}\label{example-2-ols-using-matrix-algebra}}

With more than one regressor, we need to use matrix algebra to perform
the OLS estimation. For demonstration purposes, we continue using the
bivariate data generated above, but now we write the OLS regression as
\[
y_i = \mathbf{x}_i' \gamma + u_i
\] where \(\gamma = (\alpha, \beta)\), and the regressors now contain a
constant, \(\mathbf{x_i} = (1, x_i)'\). As stated above, the OLS
estimator is given by \[
\def\bX{\mathbf{X}}
\widehat{\gamma} = \left(\bX'\bX\right)^{-1}\bX'\mathbf{y}
\]

\hypertarget{naive-solution}{%
\paragraph{Naive solution}\label{naive-solution}}

You might be tempted to solve the above equation system by explicitly
computing the inverse of \(\mathbf{X}'\mathbf{X}\) using NumPy's
\href{https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html}{\texttt{inv()}}
like this:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k+kn}{import} \PY{n}{inv}

\PY{c+c1}{\PYZsh{} We transpose the return value and unpack individual rows into X and Y}
\PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{draw\PYZus{}bivariate\PYZus{}sample}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{std}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{Nobs}\PY{p}{)}\PY{o}{.}\PY{n}{T}

\PY{c+c1}{\PYZsh{} Create vector of ones (required to estimate the intercept)}
\PY{n}{ones} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Prepend constant to vector of regressors to create regressor matrix X}
\PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{ones}\PY{p}{,} \PY{n}{x}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Compute inverse of X\PYZsq{}X}
\PY{n}{XXinv} \PY{o}{=} \PY{n}{inv}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{X}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Explicitly computed (X}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{X)\PYZca{}(\PYZhy{}1):}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{XXinv}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Compute naive estimate of gamma}
\PY{n}{gamma\PYZus{}naive} \PY{o}{=} \PY{n}{XXinv} \PY{o}{@} \PY{n}{X}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{y}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Naive estimate of gamma: }\PY{l+s+si}{\PYZob{}}\PY{n}{gamma\PYZus{}naive}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Explicitly computed (X'X)\^{}(-1):
[[0.05633363 0.04521468]
 [0.04521468 0.04412275]]
Naive estimate of gamma: [-0.20352351 -1.3889613 ]
    \end{Verbatim}

    This might seems like a straightforward way to implement OLS, but in
practice you should \emph{never} do this. Explicitly taking the inverse
of a matrix to solve an equation system is rarely a good idea and
numerically unstable, even though in this particular case it yields the
same result!

    \hypertarget{solving-as-a-linear-equation-system}{%
\paragraph{Solving as a linear equation
system}\label{solving-as-a-linear-equation-system}}

One numerically acceptable way to run OLS is to view it as a linear
equation system. Recall that a linear equation system can be written in
matrix notation as \[
\mathbf{A} \mathbf{z} = \mathbf{b}
\] where \(\mathbf{A} \in \mathbb{R}^{k \times k}\) is a coefficient
matrix of full rank, \(\mathbf{b} \in \mathbb{R}^k\) is a vector, and
\(\mathbf{z} \in \mathbb{R}^k\) is a vector of \(k\) unknows we want to
solve for. The OLS estimator can be written in this form if we set \[
\begin{aligned}
    \mathbf{A} &= \mathbf{X}'\mathbf{X} \\
    \mathbf{b} &= \mathbf{X}'\mathbf{y} \\
    \mathbf{z} &= \widehat{\gamma}
\end{aligned}
\] so that we have \[
    (\mathbf{X}'\mathbf{X})\widehat{\gamma} = \mathbf{X}'\mathbf{y}
\] We can use NumPy's
\href{https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html}{\texttt{solve()}}
to find \(\widehat{\gamma}\):

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k+kn}{import} \PY{n}{solve}

\PY{c+c1}{\PYZsh{} Compute X\PYZsq{}X}
\PY{n}{A} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{X}
\PY{c+c1}{\PYZsh{} Compute X\PYZsq{}y}
\PY{n}{b} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{y}

\PY{c+c1}{\PYZsh{} Solve for coefficient vector}
\PY{n}{gamma\PYZus{}solve} \PY{o}{=} \PY{n}{solve}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{b}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimate of gamma using solve(): }\PY{l+s+si}{\PYZob{}}\PY{n}{gamma\PYZus{}solve}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estimate of gamma using solve(): [-0.20352351 -1.3889613 ]
    \end{Verbatim}

    Of course, running OLS (or equivalently: solving an overdetermined
linear equation system) is a common task, so NumPy has the function
\href{https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html}{\texttt{lstsq()}}
which allows you do to it without explicitly computing
\(\mathbf{X}'\mathbf{X}\) or \(\mathbf{X}'\mathbf{y}\):

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k+kn}{import} \PY{n}{lstsq}

\PY{c+c1}{\PYZsh{} Estimate using lstsq(). Pass rcond=None to suppress a warning.}
\PY{n}{gamma\PYZus{}lstsq}\PY{p}{,} \PY{o}{*}\PY{n}{rest} \PY{o}{=} \PY{n}{lstsq}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{rcond}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimate of gamma using lstsq(): }\PY{l+s+si}{\PYZob{}}\PY{n}{gamma\PYZus{}lstsq}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estimate of gamma using lstsq(): [-0.20352351 -1.3889613 ]
    \end{Verbatim}

    \hypertarget{example-3-implementing-ols-yourself}{%
\subsubsection{Example 3: Implementing OLS
yourself}\label{example-3-implementing-ols-yourself}}

NumPy's \texttt{lstsq()} uses SVD to compute the solution. Since we
covered SVD in a previous exercise, we already have the tools to build
our own implementation.

Recall that SVD factorises a regressor matrix \(\mathbf{X}\) into three
matrices, \[
\mathbf{X} = \mathbf{U} \Sigma \mathbf{V}'
\] We can use the orthogonality properties of \(\mathbf{U}\) and
\(\mathbf{V}\) described above to transform the OLS estimator. We will
be using the fact that the transpose of \(\mathbf{X}\) is \[
\mathbf{X}' = \mathbf{V} \Sigma' \mathbf{U}' = 
    \mathbf{V} \Sigma \mathbf{U}'
\] which follows since \(\Sigma\) is a diagonal (and thus symmetric)
matrix. The OLS estimator can then be expressed as follows: \[
\def\bV{\mathbf{V}}
\def\bX{\mathbf{X}}
\def\bU{\mathbf{U}}
\def\bY{\mathbf{y}}
\def\ident{\mathbf{I}}
\begin{aligned}
\widehat{\gamma} &= (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y} \\
    &= \left(\bV \Sigma \bU' \bU \Sigma \bV' \right)^{-1}
        \bV \Sigma \bU' \bY \\
    &= \left(\bV \Sigma \ident_k \Sigma \bV' \right)^{-1}
        \bV \Sigma \bU' \bY \\
    &= \left(\bV \Sigma^2 \bV' \right)^{-1}
        \bV \Sigma \bU' \bY \\
\end{aligned}
\] This follows since \(\mathbf{U}'\mathbf{U} = \mathbf{I}_k\) is an
identity matrix where \(k=2\) is the number of coefficients we are
estimating. Next, we can compute the inverse using the orthogonality
properties of \(\mathbf{V}\), \[
\def\bV{\mathbf{V}}
\begin{aligned}
    \bV\bV' &= \bV'\bV = \mathbf{I} \\
    \bV' &= \bV^{-1}
\end{aligned}
\] Therefore, \[
\def\bV{\mathbf{V}}
\begin{aligned}
    \left(\bV \Sigma^2 \bV' \right)^{-1} =
    (\bV')^{-1} \Sigma^{-2} \bV^{-1} = \bV \Sigma^{-2} \bV'
\end{aligned}
\] Plugging this into the expression for the OLS estimator, we see that
\[
\def\bV{\mathbf{V}}
\def\bU{\mathbf{U}}
\def\bY{\mathbf{y}}
\def\ident{\mathbf{I}}
\begin{aligned}
\widehat{\gamma} 
    &= \left(\bV \Sigma^2 \bV' \right)^{-1} \bV \Sigma \bU' \bY \\
    &= \bV \Sigma^{-2} \bV' \bV \Sigma \bU' \bY \\
    &= \bV \Sigma^{-2} \ident_k \Sigma \bU' \bY \\
    &= \bV \Sigma^{-1} \bU' \bY
\end{aligned}
\] Why is this preferable to the original expression? Since \(\Sigma\)
is a diagonal matrix, its inverse is trivially computed as the
element-wise inverse of its diagonal elements!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k+kn}{import} \PY{n}{svd}

\PY{c+c1}{\PYZsh{} Request \PYZdq{}compact\PYZdq{} SVD, we don\PYZsq{}t need the full matrix U.}
\PY{n}{U}\PY{p}{,} \PY{n}{S}\PY{p}{,} \PY{n}{Vt} \PY{o}{=} \PY{n}{svd}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{full\PYZus{}matrices}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Note that S returned by svd() is a vector that contains the diagonal}
\PY{c+c1}{\PYZsh{} of the matrix Sigma.}
\PY{n}{gamma\PYZus{}svd} \PY{o}{=} \PY{n}{Vt}\PY{o}{.}\PY{n}{T} \PY{o}{*} \PY{n}{S}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{@} \PY{n}{U}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{y}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimate of gamma using SVD: }\PY{l+s+si}{\PYZob{}}\PY{n}{gamma\PYZus{}svd}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estimate of gamma using SVD: [-0.20352351 -1.3889613 ]
    \end{Verbatim}

    \hypertarget{example-4-ols-standard-errors}{%
\subsubsection{Example 4: OLS standard
errors}\label{example-4-ols-standard-errors}}

All of the above methods only computed the \emph{point estimates}, \ie,
the coefficient vector. Usually, we are interested in performing
inference, \ie, testing some hypothesis, for example whether our
estimate is significantly different from zero. To this end, we need to
compute standard errors which reflect the sampling uncertainty of our
estimates.

Under the assumption of
\href{https://en.wikipedia.org/wiki/Homoscedasticity}{homoskedastic}
errors, the variance-covariance matrix of the OLS estimator
\(\widehat{\gamma}\) is given by the expression \[
\begin{aligned}
Var(\widehat{\gamma}) &= \widehat{\sigma}^2 \left(\mathbf{X}'\mathbf{X}\right)^{-1} \\
\widehat{\sigma}^2 &= \frac{1}{n-k} \sum_{i=1}^n \widehat{u}_i^2
\end{aligned}
\] where \(\widehat{\sigma}^2\) is the sample variance of the residuals
(recall that we have included an intercept in the model, so the mean of
\(\widehat{u}_i\) is zero!). Note the degree-of-freedom correction in
the denominator for a model with \(k\) parameters (including any
intercept).

Luckily, we can directly use our insights from the previous section and
instead of computing \(\left(\mathbf{X}'\mathbf{X}\right)^{-1}\)
directly (which is numerically undesirable), we can rewrite it using the
SVD factorisation as follows: \[
\def\bV{\mathbf{V}}
\def\bX{\mathbf{X}}
\def\bU{\mathbf{U}}
\def\bY{\mathbf{y}}
\def\ident{\mathbf{I}}
\begin{aligned}
(\bX'\bX)^{-1}
    &= \left(\bV \Sigma \bU' \bU \Sigma \bV' \right)^{-1} \\
    &= \left(\bV \Sigma \ident_k \Sigma \bV' \right)^{-1} \\
    &= \left(\bV \Sigma^2 \bV' \right)^{-1} \\
    &= \bV \Sigma^{-2} \bV'
\end{aligned}
\]

Extending the code from above, we can now compute the point estimate and
the standard errors:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k+kn}{import} \PY{n}{svd}

\PY{c+c1}{\PYZsh{} Request \PYZdq{}compact\PYZdq{} SVD, we don\PYZsq{}t need the full matrix U.}
\PY{n}{U}\PY{p}{,} \PY{n}{S}\PY{p}{,} \PY{n}{Vt} \PY{o}{=} \PY{n}{svd}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{full\PYZus{}matrices}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Compute point estimate as before}
\PY{n}{gamma} \PY{o}{=} \PY{n}{Vt}\PY{o}{.}\PY{n}{T} \PY{o}{*} \PY{n}{S}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{@} \PY{n}{U}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{y}

\PY{c+c1}{\PYZsh{} Compute (X\PYZsq{}X)\PYZca{}\PYZhy{}1 }
\PY{n}{XXinv} \PY{o}{=} \PY{n}{Vt}\PY{o}{.}\PY{n}{T} \PY{o}{*} \PY{n}{S}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{@} \PY{n}{Vt}

\PY{c+c1}{\PYZsh{} Residuals are given as u = y \PYZhy{} X*gamma}
\PY{n}{residuals} \PY{o}{=} \PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{X} \PY{o}{@} \PY{n}{gamma}

\PY{c+c1}{\PYZsh{} Variance of residuals}
\PY{n}{k} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{var\PYZus{}u} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{residuals}\PY{p}{,} \PY{n}{ddof}\PY{o}{=}\PY{n}{k}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Variance\PYZhy{}covariance matrix of estimates}
\PY{n}{var\PYZus{}gamma} \PY{o}{=} \PY{n}{var\PYZus{}u} \PY{o}{*} \PY{n}{XXinv}

\PY{c+c1}{\PYZsh{} Standard errors are square roots of diagonal elements of Var(gamma)}
\PY{n}{gamma\PYZus{}se} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{var\PYZus{}gamma}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Point estimate of gamma: }\PY{l+s+si}{\PYZob{}}\PY{n}{gamma}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Standard errors of gamma: }\PY{l+s+si}{\PYZob{}}\PY{n}{gamma\PYZus{}se}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Point estimate of gamma: [-0.20352351 -1.3889613 ]
Standard errors of gamma: [0.26527596 0.23477147]
    \end{Verbatim}

    \hypertarget{example-5-complete-ols-estimation-routine}{%
\subsubsection{Example 5: Complete OLS estimation
routine}\label{example-5-complete-ols-estimation-routine}}

We can combine all our previous code and encapsulate it in a function
called \texttt{ols}, which makes sure the input data are NumPy arrays
and have the same number of observations. We also add the optional
parameter \texttt{add\_const} which allows callers to automatically
include a constant in the model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{ols}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{add\PYZus{}const}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Run the OLS regression y = X * beta + u}
\PY{l+s+sd}{    and return the estimated coefficients beta and their variance\PYZhy{}covariance}
\PY{l+s+sd}{    matrix.}

\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    X : array\PYZus{}like}
\PY{l+s+sd}{        Matrix (or vector) of regressors}
\PY{l+s+sd}{    y : array\PYZus{}like}
\PY{l+s+sd}{        Vector of observations of dependent variable}
\PY{l+s+sd}{    add\PYZus{}const : bool, optional}
\PY{l+s+sd}{        If True, prepend a constant to regressor matrix X.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k+kn}{import} \PY{n}{svd}

    \PY{c+c1}{\PYZsh{} Make sure that y is a one\PYZhy{}dimensional array}
    \PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{atleast\PYZus{}1d}\PY{p}{(}\PY{n}{y}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Number of obs.}
    \PY{n}{Nobs} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{size}

    \PY{c+c1}{\PYZsh{} Make sure that X is a matrix and the leading dimension contains the}
    \PY{c+c1}{\PYZsh{} observations}
    \PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{atleast\PYZus{}2d}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{Nobs}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Check that arrays are of conformable dimensions, and raise an exception}
    \PY{c+c1}{\PYZsh{} if that is not the case}
    \PY{k}{if} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{!=} \PY{n}{Nobs}\PY{p}{:}
        \PY{k}{raise} \PY{n+ne}{ValueError}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Non\PYZhy{}conformable arrays X and y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Check whether we need to prepend a constant}
    \PY{k}{if} \PY{n}{add\PYZus{}const}\PY{p}{:}
        \PY{n}{ones} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{Nobs}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
        \PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{ones}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Request \PYZdq{}compact\PYZdq{} SVD, we don\PYZsq{}t need the full matrix U.}
    \PY{n}{U}\PY{p}{,} \PY{n}{S}\PY{p}{,} \PY{n}{Vt} \PY{o}{=} \PY{n}{svd}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{full\PYZus{}matrices}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Compute point estimate using SVD factorisation}
    \PY{n}{beta} \PY{o}{=} \PY{n}{Vt}\PY{o}{.}\PY{n}{T} \PY{o}{*} \PY{n}{S}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{@} \PY{n}{U}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{y}

    \PY{c+c1}{\PYZsh{} Compute (X\PYZsq{}X)\PYZca{}\PYZhy{}1 using SVD factorisation}
    \PY{n}{XXinv} \PY{o}{=} \PY{n}{Vt}\PY{o}{.}\PY{n}{T} \PY{o}{*} \PY{n}{S}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{@} \PY{n}{Vt}

    \PY{c+c1}{\PYZsh{} Residuals are given as u = y \PYZhy{} X*beta}
    \PY{n}{residuals} \PY{o}{=} \PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{X} \PY{o}{@} \PY{n}{beta}

    \PY{c+c1}{\PYZsh{} Number of model parameters}
    \PY{n}{k} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Variance of residuals}
    \PY{n}{var\PYZus{}u} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{residuals}\PY{p}{,} \PY{n}{ddof}\PY{o}{=}\PY{n}{k}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Variance\PYZhy{}covariance matrix of estimates}
    \PY{n}{var\PYZus{}beta} \PY{o}{=} \PY{n}{var\PYZus{}u} \PY{o}{*} \PY{n}{XXinv}

    \PY{k}{return} \PY{n}{beta}\PY{p}{,} \PY{n}{var\PYZus{}beta}
\end{Verbatim}
\end{tcolorbox}

    We can now draw our random sample and repeat the OLS estimation using
only a few lines of code!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Test our custom OLS function}

\PY{c+c1}{\PYZsh{} Draw random sample, split into x and y}
\PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{draw\PYZus{}bivariate\PYZus{}sample}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{std}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{Nobs}\PY{p}{)}\PY{o}{.}\PY{n}{T}

\PY{c+c1}{\PYZsh{} Call OLS estimator. Note that we don\PYZsq{}t need to manually add a constant!}
\PY{n}{beta}\PY{p}{,} \PY{n}{vcv} \PY{o}{=} \PY{n}{ols}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{add\PYZus{}const}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Compute standard errors}
\PY{n}{beta\PYZus{}SE} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{vcv}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Point estimate: }\PY{l+s+si}{\PYZob{}}\PY{n}{beta}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Standard errors: }\PY{l+s+si}{\PYZob{}}\PY{n}{beta\PYZus{}SE}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Point estimate: [-0.20352351 -1.3889613 ]
Standard errors: [0.26527596 0.23477147]
    \end{Verbatim}


    \hypertarget{ols-using-housing-data}{%
\subsection{OLS using housing data}\label{ols-using-housing-data}}

We now proceed to run a more meaningful regression using the
\texttt{ols()} function developed above. To this end, we use monthly
observations from the file \texttt{HOUSING.csv} which contains various
variables related to the US housing market. In particular, we will take
the number of housing unit construction starts (variable
\texttt{NHSTART}) in a given month and regress it on the average sales
price of new homes (variable \texttt{ASPNHS}) lagged by 3, 6 and 12
months. We run the regression in logs, so the estimated coefficient
should be interpreted as elasticities.

If you are familiar with Stata, the regression we are trying to run will
look like this:
        {\footnotesize
    \begin{verbatim}
. regress log_nhstart L3.log_aspnhs L6.log_aspnhs L12.log_aspnhs if year >= 2000

      Source |       SS           df       MS      Number of obs   =       259
-------------+----------------------------------   F(3, 255)       =     19.27
       Model |  7.50833492         3  2.50277831   Prob > F        =    0.0000
    Residual |  33.1247614       255  .129901025   R-squared       =    0.1848
-------------+----------------------------------   Adj R-squared   =    0.1752
       Total |  40.6330963       258  .157492621   Root MSE        =    .36042

------------------------------------------------------------------------------
 log_nhstart |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
  log_aspnhs |
         L3. |   2.293265   .5024764     4.56   0.000     1.303733    3.282797
         L6. |   .8001798   .5538507     1.44   0.150    -.2905242    1.890884
        L12. |   -1.94804   .4377757    -4.45   0.000    -2.810156   -1.085924
             |
       _cons |  -6.484041   2.783441    -2.33   0.021     -11.9655   -1.002581
------------------------------------------------------------------------------
\end{verbatim}
    }
    \hypertarget{load-and-visually-inspect-the-data}{%
\subsubsection{Load and visually inspect the
data}\label{load-and-visually-inspect-the-data}}

We first load and inspect the data using pandas's
\href{https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html}{\texttt{read\_csv()}}
function:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}

\PY{n}{file} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/HOUSING.csv}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{file}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Inspect first and last rows of the DataFrame}
\PY{n}{df}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     Year  Month  NHSTART  MORTGAGE30  CSHPRICE  HSN1F    ASPNHS    CPI  \textbackslash{}
0    1975      1   1032.0         9.4       NaN  416.0   39500.0   52.3
1    1975      2    904.0         9.1       NaN  422.0   40600.0   52.6
2    1975      3    993.0         8.9       NaN  477.0   42100.0   52.8
3    1975      4   1005.0         8.8       NaN  543.0   42000.0   53.0
4    1975      5   1121.0         8.9       NaN  579.0   43200.0   53.1
..    {\ldots}    {\ldots}      {\ldots}         {\ldots}       {\ldots}    {\ldots}       {\ldots}    {\ldots}
554  2021      3   1725.0         3.1     245.5  873.0  414700.0  264.8
555  2021      4   1514.0         3.1     249.8  796.0  434800.0  266.8
556  2021      5   1594.0         3.0     254.4  720.0  442500.0  268.6
557  2021      6   1650.0         3.0     259.0  701.0  429600.0  271.0
558  2021      7   1534.0         2.9       NaN  708.0  446000.0  272.3

     HSUPPLY
0        9.9
1       10.4
2        8.9
3        7.2
4        6.8
..       {\ldots}
554      4.2
555      4.8
556      5.5
557      6.0
558      6.2

[559 rows x 9 columns]
\end{Verbatim}
\end{tcolorbox}
        
    The data contains several variables which we won't be using in this
analysis, such as the Case-Shiller house price index (\texttt{CSHPRICE})
which has missing values for some of the earlier dates (missing values
are denoted as \texttt{NaN}).

Let's first plot the bivariate relationship between new house starts and
the (concurrent) average sales price. The price is in current dollars,
so we first need to deflate it (using the \texttt{CPI}) to make the
values comparable across this 45-year period.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{c+c1}{\PYZsh{} Convert average selling price to 1982\PYZhy{}1984 dollars.}
\PY{c+c1}{\PYZsh{} The value of 100 corresponds to the average price level between 1982\PYZhy{}1984.}
\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ASPNHS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{/}\PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CPI}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{/} \PY{l+m+mf}{100.0}

\PY{n}{df}\PY{o}{.}\PY{n}{plot}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{NHSTART}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ASPNHS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidths}\PY{o}{=}\PY{l+m+mf}{0.75}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{none}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{edgecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{steelblue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<AxesSubplot:xlabel='NHSTART', ylabel='ASPNHS'>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{unit11_files/unit11_48_1.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    This scatter plot looks somewhat unexpected as there seems to be no
clear relationship between housing supply and house prices. This might
be because the relationship has not remained stable over the decades
covered by our data.

To see this more clearly, we bin the time periods into five blocks and
recreate the plots using different colours:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Create 5 approximately equally\PYZhy{}sized bins based on the calendar year}
\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year\PYZus{}bin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{cut}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{labels}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot each group of years using a different color}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{colors} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}e41a1c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}377eb8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}4daf4a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}984ea3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}ff7f00}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Iterate over bins, plot each one separately}
\PY{n}{bins} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year\PYZus{}bin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}
\PY{k}{for} \PY{n+nb}{bin} \PY{o+ow}{in} \PY{n}{bins}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Restrict data set to relevant years}
    \PY{n}{df\PYZus{}i} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year\PYZus{}bin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n+nb}{bin}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Extract initial and terminal year of this block}
    \PY{n}{yfrom}\PY{p}{,} \PY{n}{yto} \PY{o}{=} \PY{n}{df\PYZus{}i}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{df\PYZus{}i}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}

    \PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df\PYZus{}i}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{NHSTART}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df\PYZus{}i}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ASPNHS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
        \PY{n}{label}\PY{o}{=}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Years }\PY{l+s+si}{\PYZob{}}\PY{n}{yfrom}\PY{l+s+si}{:}\PY{l+s+s1}{.0f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+si}{\PYZob{}}\PY{n}{yto}\PY{l+s+si}{:}\PY{l+s+s1}{.0f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
        \PY{n}{edgecolors}\PY{o}{=}\PY{n}{colors}\PY{p}{[}\PY{n+nb}{bin}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{none}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{New housing units started (in thousand)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Avg. new home sales price (in 1982\PYZhy{}84 USD)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

\PY{k}{del} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year\PYZus{}bin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{unit11_files/unit11_50_0.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    As you see, our suspicion was correct and there are clear changes across
the sample of 45 years. At this point we could do something more
elaborate, but for illustrative purposes we just restrict our analysis
to the period after the year 2000, where we have an upwards-sloping
relationship.

    \hypertarget{prepare-the-data}{%
\subsubsection{Prepare the data}\label{prepare-the-data}}

Before we can call the function \texttt{ols()}, we need to pre-process
the data so that we end up with NumPy arrays (the only type of data our
function accepts).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Keep only relevant variables, rest just clutters the DataFrame}
\PY{n}{varlist} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ASPNHS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{NHSTART}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{varlist}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Create YYYY\PYZhy{}MM date index}
\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{PeriodIndex}\PY{p}{(}\PY{n}{year}\PY{o}{=}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{month}\PY{o}{=}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{freq}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Create 3\PYZhy{}month, 6\PYZhy{}month and 12\PYZhy{}month lags of house prices}
\PY{n}{lags} \PY{o}{=} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{12}
\PY{k}{for} \PY{n}{lag} \PY{o+ow}{in} \PY{n}{lags}\PY{p}{:}
    \PY{n}{df}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{L}\PY{l+s+si}{\PYZob{}}\PY{n}{lag}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ASPNHS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ASPNHS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{shift}\PY{p}{(}\PY{n}{lag}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Restrict data to year \PYZgt{}= 2000}
\PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{2000}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Drop year, month, these are no longer needed}
\PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot first 13 rows, which clearly shows the lagged values}
\PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{13}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                ASPNHS  NHSTART       L3ASPNHS       L6ASPNHS      L12ASPNHS
Date
2000-01  118310.691081   1636.0  119095.776324  113437.312537  111050.394657
2000-02  117176.470588   1737.0  125593.824228  115559.545183  116211.293260
2000-03  119824.561404   1604.0  119727.488152  116090.584029  114866.504854
2000-04  121299.005266   1626.0  118310.691081  119095.776324  115852.923448
2000-05  116822.429907   1575.0  117176.470588  125593.824228  113192.771084
2000-06  114808.362369   1559.0  119824.561404  119727.488152  116807.228916
2000-07  117081.644470   1463.0  121299.005266  118310.691081  113437.312537
2000-08  115923.566879   1541.0  116822.429907  117176.470588  115559.545183
2000-09  119988.479263   1507.0  114808.362369  119824.561404  116090.584029
2000-10  123691.776883   1549.0  117081.644470  121299.005266  119095.776324
2000-11  120952.927669   1551.0  115923.566879  116822.429907  125593.824228
2000-12  119186.712486   1532.0  119988.479263  114808.362369  119727.488152
2001-01  119020.501139   1600.0  123691.776883  117081.644470  118310.691081
\end{Verbatim}
\end{tcolorbox}
        
    Now that we have created all the lagged variables, we drop all rows with
missing data and convert the relevant columns to NumPy arrays.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} List of variables to include in model}
\PY{n}{var\PYZus{}X} \PY{o}{=} \PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{L}\PY{l+s+si}{\PYZob{}}\PY{n}{lag}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ASPNHS}\PY{l+s+s1}{\PYZsq{}} \PY{k}{for} \PY{n}{lag} \PY{o+ow}{in} \PY{n}{lags}\PY{p}{]}
\PY{n}{var\PYZus{}y} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{NHSTART}\PY{l+s+s1}{\PYZsq{}}

\PY{c+c1}{\PYZsh{} Restrict to relevant variables}
\PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{var\PYZus{}X} \PY{o}{+} \PY{p}{[}\PY{n}{var\PYZus{}y}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} drop all rows with missing observations}
\PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Extract raw data from data frame}
\PY{n}{X} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{var\PYZus{}X}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{var\PYZus{}y}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Estimate as elasticity in logs}
\PY{n}{log\PYZus{}X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n}{log\PYZus{}y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{y}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Print first 5 observations}
\PY{n}{log\PYZus{}X}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[11.68768329, 11.63900565, 11.61773938],
       [11.74080836, 11.65754122, 11.66316531],
       [11.69297351, 11.66212606, 11.65152591],
       [11.68106942, 11.68768329, 11.66007676],
       [11.67143637, 11.74080836, 11.63684758]])
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{estimating-the-model}{%
\subsubsection{Estimating the model}\label{estimating-the-model}}

We are now ready to run the OLS regression.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Run our own ols() function. This returns the coefficient vector and the}
\PY{c+c1}{\PYZsh{} variance\PYZhy{}covariance matrix.}
\PY{n}{coefs}\PY{p}{,} \PY{n}{vcv} \PY{o}{=} \PY{n}{ols}\PY{p}{(}\PY{n}{log\PYZus{}X}\PY{p}{,} \PY{n}{log\PYZus{}y}\PY{p}{,} \PY{n}{add\PYZus{}const}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Compute standard errors from the VCV matrix}
\PY{n}{se} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{vcv}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimated coefficients: }\PY{l+s+si}{\PYZob{}}\PY{n}{coefs}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Standard errors: }\PY{l+s+si}{\PYZob{}}\PY{n}{se}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of obs: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{log\PYZus{}y}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estimated coefficients: [-6.48405842  2.2932615   0.80018127 -1.94803646]
Standard errors: [2.78344134 0.50247618 0.55385034 0.43777545]
Number of obs: 259
    \end{Verbatim}

    \hypertarget{running-ols-using-statsmodels}{%
\subsubsection{Running OLS using
statsmodels}\label{running-ols-using-statsmodels}}

As you can imagine, estimating an OLS regression is a common task so
there are packages which already implement this functionality for you.
One such package is \texttt{statsmodels}, which we will now use to
verify our results.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}

\PY{c+c1}{\PYZsh{} Explicitly augment the regressor matrix with a constant}
\PY{n}{log\PYZus{}X1} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{log\PYZus{}X}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define the linear model}
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{log\PYZus{}y}\PY{p}{,} \PY{n}{log\PYZus{}X1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Estimate the model}
\PY{n}{result} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Print a summary of the results}
\PY{n}{result}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<class 'statsmodels.iolib.summary.Summary'>
"""
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.185
Model:                            OLS   Adj. R-squared:                  0.175
Method:                 Least Squares   F-statistic:                     19.27
Date:                Fri, 17 Sep 2021   Prob (F-statistic):           2.72e-11
Time:                        15:10:21   Log-Likelihood:                -101.18
No. Observations:                 259   AIC:                             210.4
Df Residuals:                     255   BIC:                             224.6
Df Model:                           3
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -6.4841      2.783     -2.330      0.021     -11.966      -1.003
x1             2.2933      0.502      4.564      0.000       1.304       3.283
x2             0.8002      0.554      1.445      0.150      -0.291       1.891
x3            -1.9480      0.438     -4.450      0.000      -2.810      -1.086
==============================================================================
Omnibus:                       31.603   Durbin-Watson:                   0.184
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                9.818
Skew:                          -0.144   Prob(JB):                      0.00738
Kurtosis:                       2.090   Cond. No.                     2.55e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
[2] The condition number is large, 2.55e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
"""
\end{Verbatim}
\end{tcolorbox}
        
    As you can see, the point estimates and standard errors are exactly the
same as the ones we computed.

As for the interpretation, the regression says that a 1\% increase in
the average sales price is associated with a 2.3\% increase in new house
construction starts in three months time. The elasticity is only 0.8\%
if we consider a lag of 6 months (albeit not statistically significant),
and even reverses its sign at a 12-month horizon. This interpretation of
course assumes that prices are independent over time, which is not
overly plausible.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
